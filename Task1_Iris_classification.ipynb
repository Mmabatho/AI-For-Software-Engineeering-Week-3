{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXUPuDVLaiMaWSuF52hPhY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mmabatho/AI-For-Software-Engineeering-Week-3/blob/main/Task1_Iris_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01adf3e5-9088-4626-90d2-2f44503e91ae",
        "id": "rwVjj03wY-Mu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "IRIS SPECIES CLASSIFICATION WITH DECISION TREE\n",
            "============================================================\n",
            "\n",
            "1. LOADING THE IRIS DATASET\n",
            "------------------------------\n",
            "Dataset shape: (150, 6)\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Target classes: [np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
            "\n",
            "First 5 rows:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   species species_name  \n",
            "0        0       setosa  \n",
            "1        0       setosa  \n",
            "2        0       setosa  \n",
            "3        0       setosa  \n",
            "4        0       setosa  \n",
            "\n",
            "\n",
            "2. DATA EXPLORATION\n",
            "--------------------\n",
            "Dataset info:\n",
            "- Total samples: 150\n",
            "- Features: 4\n",
            "- Classes: 3\n",
            "\n",
            "Class distribution:\n",
            "species_name\n",
            "setosa        50\n",
            "versicolor    50\n",
            "virginica     50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Basic statistics:\n",
            "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
            "count         150.000000        150.000000         150.000000   \n",
            "mean            5.843333          3.057333           3.758000   \n",
            "std             0.828066          0.435866           1.765298   \n",
            "min             4.300000          2.000000           1.000000   \n",
            "25%             5.100000          2.800000           1.600000   \n",
            "50%             5.800000          3.000000           4.350000   \n",
            "75%             6.400000          3.300000           5.100000   \n",
            "max             7.900000          4.400000           6.900000   \n",
            "\n",
            "       petal width (cm)     species  \n",
            "count        150.000000  150.000000  \n",
            "mean           1.199333    1.000000  \n",
            "std            0.762238    0.819232  \n",
            "min            0.100000    0.000000  \n",
            "25%            0.300000    0.000000  \n",
            "50%            1.300000    1.000000  \n",
            "75%            1.800000    2.000000  \n",
            "max            2.500000    2.000000  \n",
            "\n",
            "Missing values per column:\n",
            "sepal length (cm)    0\n",
            "sepal width (cm)     0\n",
            "petal length (cm)    0\n",
            "petal width (cm)     0\n",
            "species              0\n",
            "species_name         0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "3. DATA PREPROCESSING\n",
            "----------------------\n",
            "Total missing values: 0\n",
            "âœ“ No missing values found - dataset is clean!\n",
            "\n",
            "Label encoding demonstration:\n",
            "Original labels: ['setosa' 'versicolor' 'virginica']\n",
            "Encoded labels: [0 1 2]\n",
            "Label mapping: {'setosa': np.int64(0), 'versicolor': np.int64(1), 'virginica': np.int64(2)}\n",
            "\n",
            "Final dataset shape:\n",
            "Features (X): (150, 4)\n",
            "Target (y): (150,)\n",
            "\n",
            "\n",
            "4. TRAIN-TEST SPLIT\n",
            "-------------------\n",
            "Training set: 120 samples\n",
            "Testing set: 30 samples\n",
            "Training set class distribution:\n",
            "  Class 0 (setosa): 40 samples\n",
            "  Class 1 (versicolor): 40 samples\n",
            "  Class 2 (virginica): 40 samples\n",
            "\n",
            "\n",
            "5. MODEL TRAINING\n",
            "----------------\n",
            "Training Decision Tree Classifier...\n",
            "Model parameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}\n",
            "âœ“ Model training completed!\n",
            "\n",
            "\n",
            "6. MODEL PREDICTION\n",
            "-----------------\n",
            "Predictions made on 30 test samples\n",
            "Sample predictions vs actual:\n",
            "  âœ“ Predicted: setosa, Actual: setosa\n",
            "  âœ“ Predicted: virginica, Actual: virginica\n",
            "  âœ“ Predicted: versicolor, Actual: versicolor\n",
            "  âœ“ Predicted: versicolor, Actual: versicolor\n",
            "  âœ“ Predicted: setosa, Actual: setosa\n",
            "  âœ“ Predicted: versicolor, Actual: versicolor\n",
            "  âœ“ Predicted: setosa, Actual: setosa\n",
            "  âœ“ Predicted: setosa, Actual: setosa\n",
            "  âœ“ Predicted: virginica, Actual: virginica\n",
            "  âœ“ Predicted: versicolor, Actual: versicolor\n",
            "\n",
            "\n",
            "7. MODEL EVALUATION\n",
            "-----------------\n",
            "Accuracy: 0.9333 (93.33%)\n",
            "\n",
            "Macro-averaged metrics:\n",
            "Precision: 0.9333\n",
            "Recall: 0.9333\n",
            "\n",
            "Weighted-averaged metrics:\n",
            "Precision: 0.9333\n",
            "Recall: 0.9333\n",
            "\n",
            "Detailed Classification Report:\n",
            "----------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.90      0.90      0.90        10\n",
            "   virginica       0.90      0.90      0.90        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.93      0.93      0.93        30\n",
            "weighted avg       0.93      0.93      0.93        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "----------------\n",
            "[[10  0  0]\n",
            " [ 0  9  1]\n",
            " [ 0  1  9]]\n",
            "\n",
            "Confusion Matrix (with labels):\n",
            "                   Pred setosa  Pred versicolor  Pred virginica\n",
            "Actual setosa               10                0               0\n",
            "Actual versicolor            0                9               1\n",
            "Actual virginica             0                1               9\n",
            "\n",
            "\n",
            "8. FEATURE IMPORTANCE\n",
            "-------------------\n",
            "Feature importance scores:\n",
            "  sepal length (cm): 0.0062\n",
            "  sepal width (cm): 0.0292\n",
            "  petal length (cm): 0.5586\n",
            "  petal width (cm): 0.4060\n",
            "\n",
            "Features ranked by importance:\n",
            "  petal length (cm): 0.5586\n",
            "  petal width (cm): 0.4060\n",
            "  sepal width (cm): 0.0292\n",
            "  sepal length (cm): 0.0062\n",
            "\n",
            "\n",
            "9. PERFORMANCE SUMMARY\n",
            "---------------------\n",
            "ðŸŽ¯ FINAL RESULTS:\n",
            "   â€¢ Accuracy: 0.9333 (93.33%)\n",
            "   â€¢ Precision (macro): 0.9333\n",
            "   â€¢ Recall (macro): 0.9333\n",
            "   â€¢ Most important feature: petal length (cm)\n",
            "   â€¢ Least important feature: sepal length (cm)\n",
            "\n",
            "Per-class performance:\n",
            "   â€¢ setosa: 1.0000 accuracy\n",
            "   â€¢ versicolor: 0.9000 accuracy\n",
            "   â€¢ virginica: 0.9000 accuracy\n",
            "\n",
            "============================================================\n",
            "ANALYSIS COMPLETE!\n",
            "============================================================\n",
            "\n",
            "ðŸ’¡ KEY INSIGHTS:\n",
            "   â€¢ The Decision Tree achieved 93.3% accuracy on the test set\n",
            "   â€¢ petal length (cm) is the most discriminative feature\n",
            "   â€¢ The model shows good performance\n",
            "   â€¢ All classes are well-balanced in the dataset\n",
            "   â€¢ Good performance, but there's room for improvement with feature engineering or different algorithms.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Classical ML with Scikit-learn: Iris Species Classification\n",
        "===========================================================\n",
        "\n",
        "This script demonstrates a complete machine learning pipeline using the Iris dataset:\n",
        "1. Data loading and exploration\n",
        "2. Data preprocessing\n",
        "3. Model training (Decision Tree)\n",
        "4. Model evaluation with multiple metrics\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IRIS SPECIES CLASSIFICATION WITH DECISION TREE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Load the Iris Dataset\n",
        "print(\"\\n1. LOADING THE IRIS DATASET\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Load the iris dataset from scikit-learn\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
        "y = iris.target  # Target: species (0: setosa, 1: versicolor, 2: virginica)\n",
        "\n",
        "# Create a DataFrame for easier manipulation\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['species'] = y\n",
        "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Features: {list(iris.feature_names)}\")\n",
        "print(f\"Target classes: {list(iris.target_names)}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 2: Data Exploration\n",
        "print(\"\\n\\n2. DATA EXPLORATION\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "print(f\"Dataset info:\")\n",
        "print(f\"- Total samples: {len(df)}\")\n",
        "print(f\"- Features: {len(iris.feature_names)}\")\n",
        "print(f\"- Classes: {len(iris.target_names)}\")\n",
        "\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df['species_name'].value_counts())\n",
        "\n",
        "print(f\"\\nBasic statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Step 3: Data Preprocessing\n",
        "print(\"\\n\\n3. DATA PREPROCESSING\")\n",
        "print(\"-\" * 22)\n",
        "\n",
        "# Check for missing values (Iris dataset is clean, but we'll demonstrate the process)\n",
        "missing_values = df.isnull().sum().sum()\n",
        "print(f\"Total missing values: {missing_values}\")\n",
        "\n",
        "if missing_values > 0:\n",
        "    print(\"Handling missing values...\")\n",
        "    # For numerical columns, we could use mean/median imputation\n",
        "    # df.fillna(df.mean(), inplace=True)\n",
        "else:\n",
        "    print(\"âœ“ No missing values found - dataset is clean!\")\n",
        "\n",
        "# Label encoding (though not needed for iris as it's already encoded)\n",
        "print(\"\\nLabel encoding demonstration:\")\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df['species_name'])\n",
        "print(f\"Original labels: {df['species_name'].unique()}\")\n",
        "print(f\"Encoded labels: {np.unique(y_encoded)}\")\n",
        "print(f\"Label mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "\n",
        "# For this example, we'll use the original numeric targets\n",
        "X_final = X  # Features\n",
        "y_final = y  # Target (already encoded as 0, 1, 2)\n",
        "\n",
        "print(f\"\\nFinal dataset shape:\")\n",
        "print(f\"Features (X): {X_final.shape}\")\n",
        "print(f\"Target (y): {y_final.shape}\")\n",
        "\n",
        "# Step 4: Train-Test Split\n",
        "print(\"\\n\\n4. TRAIN-TEST SPLIT\")\n",
        "print(\"-\" * 19)\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_final, y_final,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_final  # Ensure balanced split across classes\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "print(f\"Training set class distribution:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for i, (cls, count) in enumerate(zip(unique, counts)):\n",
        "    print(f\"  Class {cls} ({iris.target_names[cls]}): {count} samples\")\n",
        "\n",
        "# Step 5: Model Training\n",
        "print(\"\\n\\n5. MODEL TRAINING\")\n",
        "print(\"-\" * 16)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(\n",
        "    random_state=42,\n",
        "    max_depth=5,  # Prevent overfitting\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1\n",
        ")\n",
        "\n",
        "print(\"Training Decision Tree Classifier...\")\n",
        "print(f\"Model parameters: {dt_classifier.get_params()}\")\n",
        "\n",
        "# Train the model\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "print(\"âœ“ Model training completed!\")\n",
        "\n",
        "# Step 6: Model Prediction\n",
        "print(\"\\n\\n6. MODEL PREDICTION\")\n",
        "print(\"-\" * 17)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "print(f\"Predictions made on {len(y_test)} test samples\")\n",
        "print(f\"Sample predictions vs actual:\")\n",
        "for i in range(min(10, len(y_test))):\n",
        "    pred_name = iris.target_names[y_pred[i]]\n",
        "    actual_name = iris.target_names[y_test[i]]\n",
        "    status = \"âœ“\" if y_pred[i] == y_test[i] else \"âœ—\"\n",
        "    print(f\"  {status} Predicted: {pred_name}, Actual: {actual_name}\")\n",
        "\n",
        "# Step 7: Model Evaluation\n",
        "print(\"\\n\\n7. MODEL EVALUATION\")\n",
        "print(\"-\" * 17)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Calculate precision and recall\n",
        "# For multiclass classification, we need to specify the average method\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
        "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"\\nMacro-averaged metrics:\")\n",
        "print(f\"Precision: {precision_macro:.4f}\")\n",
        "print(f\"Recall: {recall_macro:.4f}\")\n",
        "\n",
        "print(f\"\\nWeighted-averaged metrics:\")\n",
        "print(f\"Precision: {precision_weighted:.4f}\")\n",
        "print(f\"Recall: {recall_weighted:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nDetailed Classification Report:\")\n",
        "print(\"-\" * 40)\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "print(report)\n",
        "\n",
        "# Confusion Matrix\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(\"-\" * 16)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Create a more readable confusion matrix\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=[f'Actual {name}' for name in iris.target_names],\n",
        "                     columns=[f'Pred {name}' for name in iris.target_names])\n",
        "print(f\"\\nConfusion Matrix (with labels):\")\n",
        "print(cm_df)\n",
        "\n",
        "# Step 8: Feature Importance\n",
        "print(\"\\n\\n8. FEATURE IMPORTANCE\")\n",
        "print(\"-\" * 19)\n",
        "\n",
        "feature_importance = dt_classifier.feature_importances_\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(\"Feature importance scores:\")\n",
        "for name, importance in zip(feature_names, feature_importance):\n",
        "    print(f\"  {name}: {importance:.4f}\")\n",
        "\n",
        "# Sort features by importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nFeatures ranked by importance:\")\n",
        "for idx, row in importance_df.iterrows():\n",
        "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "# Step 9: Model Performance Summary\n",
        "print(\"\\n\\n9. PERFORMANCE SUMMARY\")\n",
        "print(\"-\" * 21)\n",
        "\n",
        "print(\"ðŸŽ¯ FINAL RESULTS:\")\n",
        "print(f\"   â€¢ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"   â€¢ Precision (macro): {precision_macro:.4f}\")\n",
        "print(f\"   â€¢ Recall (macro): {recall_macro:.4f}\")\n",
        "print(f\"   â€¢ Most important feature: {importance_df.iloc[0]['feature']}\")\n",
        "print(f\"   â€¢ Least important feature: {importance_df.iloc[-1]['feature']}\")\n",
        "\n",
        "# Calculate per-class metrics\n",
        "print(f\"\\nPer-class performance:\")\n",
        "for i, class_name in enumerate(iris.target_names):\n",
        "    class_mask = (y_test == i)\n",
        "    if np.any(class_mask):\n",
        "        class_accuracy = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
        "        print(f\"   â€¢ {class_name}: {class_accuracy:.4f} accuracy\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Additional insights\n",
        "print(f\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
        "print(f\"   â€¢ The Decision Tree achieved {accuracy*100:.1f}% accuracy on the test set\")\n",
        "print(f\"   â€¢ {importance_df.iloc[0]['feature']} is the most discriminative feature\")\n",
        "print(f\"   â€¢ The model shows {'excellent' if accuracy > 0.95 else 'good' if accuracy > 0.85 else 'moderate'} performance\")\n",
        "print(f\"   â€¢ All classes are well-balanced in the dataset\")\n",
        "\n",
        "if accuracy == 1.0:\n",
        "    print(f\"   â€¢ Perfect classification achieved! This is expected for the Iris dataset.\")\n",
        "elif accuracy > 0.95:\n",
        "    print(f\"   â€¢ Near-perfect classification - excellent model performance!\")\n",
        "else:\n",
        "    print(f\"   â€¢ Good performance, but there's room for improvement with feature engineering or different algorithms.\")\n"
      ]
    }
  ]
}